version: '3.8'

x-airflow-common:
  &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  image: custom-airflow:2.9.1-python3.10
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__WEBSERVER__SECRET_KEY: 'my_secret_key'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    _PIP_ADDITIONAL_REQUIREMENTS: ''
    AZURE_STORAGE_KEY: ${AZURE_STORAGE_KEY}
    AZURE_SQL_USERNAME: ${AZURE_SQL_USERNAME}
    AZURE_SQL_PASSWORD: ${AZURE_SQL_PASSWORD}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./requirements.txt:/opt/airflow/requirements.txt
    - ./spark/jobs:/opt/spark/jobs
    - ./spark:/opt/spark
    - ./.env:/opt/airflow/.env
    - /var/run/docker.sock:/var/run/docker.sock
  depends_on:
    - postgres

services:
  mysql:
    image: mysql:8.0
    container_name: mysql
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: cdc_db
      MYSQL_USER: admin
      MYSQL_PASSWORD: admin
    ports:
      - "3306:3306"
    volumes:
      - ./mysql/init.sql:/docker-entrypoint-initdb.d/init.sql
    command: >
      --server-id=223344
      --log-bin=mysql-bin
      --binlog-format=ROW
      --gtid-mode=ON
      --enforce-gtid-consistency=ON
      --binlog-row-image=FULL
      --binlog_checksum=NONE

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    restart: unless-stopped
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD-SHELL", "sleep 5"]

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: unless-stopped
    ports:
      - "8082:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: "local-kafka"
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "kafka:9092"
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME: "local-connect"
      KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS: "http://debezium:8083"
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    depends_on:
      kafka:
        condition: service_healthy

  debezium:
    image: debezium/connect:2.5
    container_name: debezium
    restart: unless-stopped
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: "connect-cluster"
      CONFIG_STORAGE_TOPIC: "connect-configs"
      OFFSET_STORAGE_TOPIC: "connect-offsets"
      STATUS_STORAGE_TOPIC: "connect-status"
      CONNECTOR_JAR_PATH: /kafka/connectors
      DEBEZIUM_MYSQL_ENABLED: "true"
      DEBEZIUM_MYSQL_CONNECTOR_VERSION: "2.5"
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      MYSQL_HOSTNAME: mysql
      MYSQL_PORT: 3306
      MYSQL_USER: admin
      MYSQL_PASSWORD: admin
    ports:
      - "8083:8083"
    volumes:
      - ./kafka/debezium_mysql_connector.json:/kafka/connectors/debezium_mysql_connector.json
    healthcheck:
      test: ["CMD-SHELL", "sleep 5"]
  debezium-init:
    build:
      context: .
      dockerfile: debezium-connect.Dockerfile
    container_name: debezium-init
    restart: "no"
    depends_on:
      debezium:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "sleep 5"]

  spark-master:
    image: bitnami/spark:3.4
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - AZURE_STORAGE_KEY=${AZURE_STORAGE_KEY}
      - AZURE_SQL_USERNAME=${AZURE_SQL_USERNAME}
      - AZURE_SQL_PASSWORD=${AZURE_SQL_PASSWORD}
    ports:
      - "8080:8080"
    volumes:
      - ./spark:/opt/spark
      - ./spark/jobs:/opt/spark/jobs
    healthcheck:
      test: ["CMD-SHELL", "sleep 5"]
    command: >
      bash -c "
      pip install python-dotenv &&
      /opt/bitnami/scripts/spark/entrypoint.sh /opt/bitnami/scripts/spark/run.sh"

  spark-worker:
    image: bitnami/spark:3.4
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - AZURE_STORAGE_KEY=${AZURE_STORAGE_KEY}
      - AZURE_SQL_USERNAME=${AZURE_SQL_USERNAME}
      - AZURE_SQL_PASSWORD=${AZURE_SQL_PASSWORD}
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    volumes:
      - ./.env:/opt/spark/conf/.env
      - ./spark:/opt/spark
      - ./spark/jobs:/opt/spark/jobs
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  spark-job:
    build:
      context: ./spark
    container_name: spark-job
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark:/opt/spark
    environment:
      - SPARK_MODE=client
      - AZURE_STORAGE_KEY=${AZURE_STORAGE_KEY}
      - AZURE_SQL_USERNAME=${AZURE_SQL_USERNAME}
      - AZURE_SQL_PASSWORD=${AZURE_SQL_PASSWORD}

  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    command: webserver
    ports:
      - "8088:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    command: triggerer

  airflow-init:
    <<: *airflow-common
    command: >
      bash -c "pip install -r /opt/airflow/requirements.txt && airflow db init && airflow db upgrade && airflow users create --username admin --firstname admin --lastname admin --role Admin --email airflow@airflow.com --password admin"
    restart: "no"
    
volumes:
  mysql_data:
  kafka_data:

networks:
  default:
    name: cdc_pipeline_net
